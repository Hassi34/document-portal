{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be2df036",
   "metadata": {},
   "source": [
    "# Redis Semantic Caching within a Chain\n",
    "\n",
    "This notebook demonstrates LangChain semantic caching using Redis, and measures cache effects on an LCEL chain.\n",
    "\n",
    "Prereqs: a running Redis at `redis://localhost:6379` (or set `REDIS_URL`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13033be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain-core langchain-openai langchain-redis redis python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795583be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import (\n",
    "    AzureChatOpenAI,\n",
    "    AzureOpenAIEmbeddings,\n",
    ")\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "instance = os.getenv(\"AZURE_OPENAI_API_INSTANCE_NAME\")\n",
    "deployment = os.getenv(\"AZURE_OPENAI_API_DEPLOYMENT_NAME\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "azure_endpoint = f\"https://{instance}.openai.azure.com/\"\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    azure_deployment=deployment,\n",
    "    openai_api_version=api_version,\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "instance = os.getenv(\"AZURE_OPENAI_API_INSTANCE_NAME\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "azure_endpoint = f\"https://{instance}.openai.azure.com/\"\n",
    "# Use Azure-specific embeddings wrapper per docs\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "    openai_api_version=api_version,\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2ca0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5efcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "\n",
    "\n",
    "from langchain.globals import set_llm_cache\n",
    "# Prefer dedicated package; fallback to community cache for older versions\n",
    "from langchain_redis.cache import RedisSemanticCache\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "REDIS_URL = os.getenv('REDIS_URL', 'redis://localhost:6379')\n",
    "print(f'Connecting to Redis at: {REDIS_URL}')\n",
    "\n",
    "# Ensure your OpenAI key is available in env as OPENAI_API_KEY\n",
    "assert os.getenv('OPENAI_API_KEY'), 'Missing OPENAI_API_KEY in environment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f1ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# llm = ChatOpenAI(model='gpt-4o-mini', temperature=0.0)\n",
    "# embedding = OpenAIEmbeddings()\n",
    "# Some versions of RedisSemanticCache do not support distance_threshold/ttl/name in constructor.\n",
    "# Pass only required arguments for broad compatibility.\n",
    "set_llm_cache(RedisSemanticCache(redis_url=REDIS_URL, embeddings=embedding, distance_threshold=0.2, ttl=3600, name='document-portal'))\n",
    "\n",
    "# Build a simple LCEL chain\n",
    "prompt = ChatPromptTemplate.from_messages([('system', 'Be concise.'), ('human', '{question}')])\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print('Chain ready with Redis semantic cache.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to measure cache impact\n",
    "def timed_invoke(q: str):\n",
    "    t0 = time.time()\n",
    "    out = chain.invoke({'question': q})\n",
    "    dt = time.time() - t0\n",
    "    return out, dt\n",
    "\n",
    "q1 = 'What is the capital of France?'\n",
    "out1, t1 = timed_invoke(q1)\n",
    "print(f'First (uncached) call: {t1:.2f}s\\n{out1[:200]}')\n",
    "\n",
    "# Semantically similar prompt should hit the cache\n",
    "q2 = 'Which city is the capital of France?'\n",
    "out2, t2 = timed_invoke(q2)\n",
    "print(f'Second (semantic cached) call: {t2:.2f}s\\n{out2[:200]}')\n",
    "print(f'Improvement: {(t1 / max(t2, 1e-6)):.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: reconfigure cache (some versions support extra params like distance_threshold/ttl/name)\n",
    "# If your installed RedisSemanticCache exposes these, you can tune it here.\n",
    "# For compatibility, we'll just reinitialize the cache without extras.\n",
    "import dis\n",
    "from math import dist\n",
    "\n",
    "\n",
    "set_llm_cache(RedisSemanticCache(redis_url=REDIS_URL, embeddings=embedding, distance_threshold=0.2, ttl=3600, name='document-portal'))\n",
    "out3, t3 = timed_invoke(q2)\n",
    "print(f'Cache reinit done; timing: {t3:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad457e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the demo cache\n",
    "cache = RedisSemanticCache(redis_url=REDIS_URL, embeddings=embedding, distance_threshold=0.2, ttl=3600, name='document-portal')\n",
    "cache.clear()\n",
    "print('Cleared document-portal cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b930cc73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
