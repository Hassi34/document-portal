{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be2df036",
   "metadata": {},
   "source": [
    "# Redis Semantic Caching within a Chain\n",
    "\n",
    "This notebook demonstrates LangChain semantic caching using Redis, and measures cache effects on an LCEL chain.\n",
    "\n",
    "Prereqs: a running Redis at `redis://localhost:6379` (or set `REDIS_URL`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13033be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hasanain/Library/CloudStorage/OneDrive-Personal/DS/KN Ac/DocumentPortal/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -U langchain-core langchain-openai langchain-redis redis python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa5efcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Redis at: redis://localhost:6379\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.globals import set_llm_cache\n",
    "# Prefer dedicated package; fallback to community cache for older versions\n",
    "from langchain_redis.cache import RedisSemanticCache\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "REDIS_URL = os.getenv('REDIS_URL', 'redis://localhost:6379')\n",
    "print(f'Connecting to Redis at: {REDIS_URL}')\n",
    "\n",
    "# Ensure your OpenAI key is available in env as OPENAI_API_KEY\n",
    "assert os.getenv('OPENAI_API_KEY'), 'Missing OPENAI_API_KEY in environment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b1f1ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:25:40 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "22:25:40 redisvl.index.index INFO   Index already exists, not overwriting.\n",
      "Chain ready with Redis semantic cache.\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM, Embeddings, and Redis semantic cache\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0.0)\n",
    "embedding = OpenAIEmbeddings()\n",
    "# Some versions of RedisSemanticCache do not support distance_threshold/ttl/name in constructor.\n",
    "# Pass only required arguments for broad compatibility.\n",
    "set_llm_cache(RedisSemanticCache(redis_url=REDIS_URL, embeddings=embedding, distance_threshold=0.2, ttl=3600, name='document-portal'))\n",
    "\n",
    "# Build a simple LCEL chain\n",
    "prompt = ChatPromptTemplate.from_messages([('system', 'Be concise.'), ('human', '{question}')])\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print('Chain ready with Redis semantic cache.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "730a790f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:25:44 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "22:25:44 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "22:25:45 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "First (uncached) call: 1.34s\n",
      "The capital of France is Paris.\n",
      "22:25:45 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Second (semantic cached) call: 0.19s\n",
      "The capital of France is Paris.\n",
      "Improvement: 7.06x\n"
     ]
    }
   ],
   "source": [
    "# Helper to measure cache impact\n",
    "def timed_invoke(q: str):\n",
    "    t0 = time.time()\n",
    "    out = chain.invoke({'question': q})\n",
    "    dt = time.time() - t0\n",
    "    return out, dt\n",
    "\n",
    "q1 = 'What is the capital of France?'\n",
    "out1, t1 = timed_invoke(q1)\n",
    "print(f'First (uncached) call: {t1:.2f}s\\n{out1[:200]}')\n",
    "\n",
    "# Semantically similar prompt should hit the cache\n",
    "q2 = 'Which city is the capital of France?'\n",
    "out2, t2 = timed_invoke(q2)\n",
    "print(f'Second (semantic cached) call: {t2:.2f}s\\n{out2[:200]}')\n",
    "print(f'Improvement: {(t1 / max(t2, 1e-6)):.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: reconfigure cache (some versions support extra params like distance_threshold/ttl/name)\n",
    "# If your installed RedisSemanticCache exposes these, you can tune it here.\n",
    "# For compatibility, we'll just reinitialize the cache without extras.\n",
    "import dis\n",
    "from math import dist\n",
    "\n",
    "\n",
    "set_llm_cache(RedisSemanticCache(redis_url=REDIS_URL, embeddings=embedding, distance_threshold=0.2, ttl=3600, name='document-portal'))\n",
    "out3, t3 = timed_invoke(q2)\n",
    "print(f'Cache reinit done; timing: {t3:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad457e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the demo cache\n",
    "cache = RedisSemanticCache(redis_url=REDIS_URL, embeddings=embedding, distance_threshold=0.2, ttl=3600, name='document-portal')\n",
    "cache.clear()\n",
    "print('Cleared document-portal cache')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
