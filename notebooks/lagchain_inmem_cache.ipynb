{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a9b4f0",
   "metadata": {},
   "source": [
    "#### Cache Using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c707c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902b2c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized successfully.\n",
      "content='The capital of France is Paris.' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'} id='run--0d1b2fc5-f813-4fb4-8256-78ffc3e59153-0'\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=1000,\n",
    "    streaming=True,\n",
    "    verbose=True,\n",
    "    request_timeout=60,\n",
    "    max_retries=3\n",
    ")\n",
    "print(\"LLM initialized successfully.\")\n",
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9e2098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebuggableCache(InMemoryCache):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._cache: Dict[Tuple[str, str], Any] = {}\n",
    "\n",
    "    def lookup(self, prompt: str, llm_string: str):\n",
    "        return self._cache.get((prompt, llm_string))\n",
    "\n",
    "    def update(self, prompt: str, llm_string: str, return_val: Any):\n",
    "        self._cache[(prompt, llm_string)] = return_val\n",
    "\n",
    "    def view_cache(self):  # this is our custom method\n",
    "        return self._cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdaf2fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg_cache = DebuggableCache()\n",
    "set_llm_cache(dbg_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebac754f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'}, id='run--9550fe50-94c6-40c0-9831-2465b27bf271-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20262735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cache Contents:\n",
      "Prompt: [{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"messages\", \"HumanMessage\"], \"kwargs\": {\"content\": \"What is the capital of France?\", \"type\": \"human\"}}] | Cached Output: [ChatGeneration(text='The capital of France is Paris.', generation_info={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'}, message=AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'}, id='run--9550fe50-94c6-40c0-9831-2465b27bf271-0'))]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCache Contents:\")\n",
    "for k, v in dbg_cache.view_cache().items():\n",
    "    print(f\"Prompt: {k[0]} | Cached Output: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6644a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2042d738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cache Contents:\n",
      "Prompt: [{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"messages\", \"HumanMessage\"], \"kwargs\": {\"content\": \"What is the capital of France?\", \"type\": \"human\"}}] | Cached Output: [ChatGeneration(text='The capital of France is Paris.', generation_info={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'}, message=AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'}, id='run--9550fe50-94c6-40c0-9831-2465b27bf271-0', usage_metadata={'total_cost': 0}))]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCache Contents:\")\n",
    "for k, v in dbg_cache.view_cache().items():\n",
    "    print(f\"Prompt: {k[0]} | Cached Output: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e7e9ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: content='The capital of the United States is Washington, D.C.' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'} id='run--12d26486-2309-4965-8b0b-44ed7239d892-0'\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"What is the capital of United States?\")\n",
    "print(\"LLM Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1522d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cache Contents:\n",
      "Prompt: [{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"messages\", \"HumanMessage\"], \"kwargs\": {\"content\": \"What is the capital of France?\", \"type\": \"human\"}}] | Cached Output: [ChatGeneration(text='The capital of France is Paris.', generation_info={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'}, message=AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'}, id='run--9550fe50-94c6-40c0-9831-2465b27bf271-0', usage_metadata={'total_cost': 0}))]\n",
      "Prompt: [{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"messages\", \"HumanMessage\"], \"kwargs\": {\"content\": \"What is the capital of india?\", \"type\": \"human\"}}] | Cached Output: [ChatGeneration(text='The capital of India is New Delhi.', generation_info={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'}, message=AIMessage(content='The capital of India is New Delhi.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'}, id='run--edcfed1f-38ec-4187-abcf-097d91625395-0'))]\n",
      "Prompt: [{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"messages\", \"HumanMessage\"], \"kwargs\": {\"content\": \"What is the capital of United States?\", \"type\": \"human\"}}] | Cached Output: [ChatGeneration(text='The capital of the United States is Washington, D.C.', generation_info={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'}, message=AIMessage(content='The capital of the United States is Washington, D.C.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'}, id='run--12d26486-2309-4965-8b0b-44ed7239d892-0'))]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCache Contents:\")\n",
    "for k, v in dbg_cache.view_cache().items():\n",
    "    print(f\"Prompt: {k[0]} | Cached Output: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbde8404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: content='The capital of the United States is Washington, D.C.' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'} id='run--12d26486-2309-4965-8b0b-44ed7239d892-0' usage_metadata={'total_cost': 0}\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"What is the capital of United States?\")\n",
    "print(\"LLM Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "124618cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: content='Science is a vast field of study that encompasses a wide range of disciplines, including biology, chemistry, physics, and astronomy. It is the systematic study of the natural world through observation and experimentation, with the goal of understanding how the world works and why things happen the way they do. Science has played a crucial role in shaping our understanding of the universe and has led to countless technological advancements that have improved our quality of life.\\n\\nOne of the key components of science is mathematics, which serves as the language of science and provides the tools necessary for making sense of the natural world. Mathematics is the study of numbers, quantities, shapes, and patterns, and it is used in science to describe and analyze the relationships between different variables. Without mathematics, it would be impossible to quantify and measure the phenomena that scientists observe, making it an essential tool for conducting scientific research.\\n\\nMathematics is particularly important in the field of physics, where it is used to formulate and solve complex equations that describe the behavior of particles and forces in the universe. For example, the laws of motion formulated by Sir Isaac Newton are based on mathematical principles, and they have been used to predict the motion of objects in space and on Earth with remarkable accuracy. Similarly, the theory of relativity developed by Albert Einstein relies heavily on mathematical concepts, such as tensors and differential equations, to describe the nature of space and time.\\n\\nIn biology, mathematics is used to model and analyze biological systems, such as population dynamics, genetic inheritance, and ecological interactions. By using mathematical models, biologists can make predictions about how organisms will respond to changes in their environment and how diseases will spread through a population. Mathematics is also used in chemistry to calculate the properties of chemical compounds and to understand the behavior of atoms and molecules.\\n\\nIn astronomy, mathematics is essential for making precise measurements of celestial objects and for predicting the movements of planets and stars. Astronomers use mathematical formulas to calculate the distances to stars, the sizes of galaxies, and the orbits of comets and asteroids. Without mathematics, it would be impossible to make sense of the vast distances and timescales involved in the study of the universe.\\n\\nOverall, mathematics plays a crucial role in science by providing the tools necessary for making sense of the natural world and for conducting research. Without mathematics, scientists would be unable to quantify and analyze the phenomena they observe, making it an indispensable part of the scientific process. As our understanding of the universe continues to grow, mathematics will remain a vital tool for unlocking the mysteries of the natural world and for making new discoveries that will shape the future of science.' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'} id='run--d741ccc0-29a8-4f6b-82d7-ea016559fe63-0'\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"give me 1000 lines of essay on science and give the importance of it regarding mathematics?\")\n",
    "print(\"LLM Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b50fe0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Why did LangChain and ECS break up? Because they couldn't find a common language to communicate in!\n",
      "Token Usage Stats: Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 0\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "with get_openai_callback() as cb:\n",
    "    response = llm.invoke(\"Tell me a joke about LangChain and ECS\")\n",
    "    print(\"Response:\", response.content)\n",
    "    print(\"Token Usage Stats:\", cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "678a0d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is Retrieval-Augmented Generation?\n",
      "A: Retrieval-Augmented Generation (RAG) is a natural language processing model that combines the capabilities of both retrieval-based and generation-based models. In RAG, a retrieval component is used to retrieve relevant information from a large database or knowledge base, which is then used by a generation component to generate a response or answer to a given query or prompt. This approach allows the model to leverage the benefits of both retrieval and generation models, resulting in more accurate and contextually relevant responses. RAG has been shown to outperform traditional generation models in tasks such as question answering and dialogue generation.\n",
      "\n",
      "Q: How does FAISS indexing work?\n",
      "A: FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of large datasets. It works by creating an index that organizes the data in a way that makes it faster to search for similar items.\n",
      "\n",
      "The indexing process involves several steps:\n",
      "\n",
      "1. Data preprocessing: The input data is preprocessed to convert it into a format that is suitable for indexing. This may involve normalizing the data, reducing dimensionality, or applying other transformations.\n",
      "\n",
      "2. Index construction: The preprocessed data is then used to build an index structure that organizes the data in a way that makes it efficient to search for similar items. FAISS supports several types of index structures, such as IVF (Inverted File with Hierarchical Clustering) and PQ (Product Quantization).\n",
      "\n",
      "3. Query processing: When a query is made to search for similar items, the index structure is used to quickly retrieve the most similar items to the query. FAISS uses efficient algorithms and data structures to speed up the search process.\n",
      "\n",
      "Overall, FAISS indexing works by organizing the data in a way that makes it faster to search for similar items, using efficient index structures and algorithms to process queries quickly and accurately.\n",
      "\n",
      "Q: What is the difference between fine-tuning and RAG?\n",
      "A: Fine-tuning refers to the process of adjusting and optimizing a pre-trained language model on a specific dataset or task to improve its performance on that particular task. This involves training the model on new data while keeping the weights of the pre-trained model fixed or making only minor adjustments.\n",
      "\n",
      "On the other hand, RAG (Retrieval-Augmented Generation) is a model architecture that combines a pre-trained language model with a retriever component to improve the generation of text. The retriever component is used to retrieve relevant information from a knowledge source, which is then used by the language model to generate text. RAG is designed to improve the quality and relevance of generated text by incorporating external knowledge into the generation process.\n",
      "\n",
      "In summary, fine-tuning involves adjusting the weights of a pre-trained language model on a specific task, while RAG is a model architecture that combines a language model with a retriever component to improve text generation by incorporating external knowledge.\n",
      "\n",
      "=== Token Usage Summary ===\n",
      "Total Tokens: 0\n",
      "Prompt Tokens: 0\n",
      "Completion Tokens: 0\n",
      "Total Cost (USD): $0.000000\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is Retrieval-Augmented Generation?\",\n",
    "    \"How does FAISS indexing work?\",\n",
    "    \"What is the difference between fine-tuning and RAG?\"\n",
    "]\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    for q in questions:\n",
    "        answer = llm.invoke(q)\n",
    "        print(f\"\\nQ: {q}\\nA: {answer.content}\")\n",
    "\n",
    "    print(\"\\n=== Token Usage Summary ===\")\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8fbea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
